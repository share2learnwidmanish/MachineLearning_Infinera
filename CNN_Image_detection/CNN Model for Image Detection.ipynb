{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fbf60e50",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from keras_preprocessing.image import ImageDataGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9581126d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.9.1'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Verify tensorflow library is 2.0 and above\n",
    "tf.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3004d57d",
   "metadata": {},
   "source": [
    "# Part-1 Data Preprocessing \n",
    "- Apply transformation on training set, Image augmentation to avoid overfitting \n",
    "    - We will use Keras APIs for data preprocessing. (https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/image/ImageDataGenerator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "59ee3fe4",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_datagen = ImageDataGenerator(\n",
    "        rescale=1./255,\n",
    "        shear_range=0.2,\n",
    "        zoom_range=0.2,\n",
    "        horizontal_flip=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8912b437",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\maagarwal\\\\Desktop\\\\IIT_Assignments\\\\pandas_handsOn\\\\CNN_Image_detection'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9c01aa34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 8000 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "# Specify the path of the training set, target image size \n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "        'dataset/training_set',\n",
    "        target_size=(64, 64),\n",
    "        batch_size=32,\n",
    "        class_mode='binary')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c92459e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scaling the test image data, will not do any other transformation\n",
    "test_datagen = ImageDataGenerator(rescale=1./255)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3c5b11f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2000 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "test_set = test_datagen.flow_from_directory(\n",
    "        'dataset/test_set',\n",
    "        target_size=(64, 64),\n",
    "        batch_size=32,\n",
    "        class_mode='binary')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db835cbc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "26562199",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn = tf.keras.models.Sequential()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afa111b3",
   "metadata": {},
   "source": [
    "# 1. Add convolutional layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bcb1868d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chose # of filters, Kernel_size(filter size), activation = 'relu'\n",
    "cnn.add(tf.keras.layers.Conv2D(filters=32,kernel_size=3, activation='relu', input_shape=[64,64,3]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79eed21c",
   "metadata": {},
   "source": [
    "# 2. Pooling layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e9b191ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn.add(tf.keras.layers.MaxPool2D(pool_size=2, strides=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0be9612",
   "metadata": {},
   "source": [
    "# 3. Add additonal convolutional layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3dff6f20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove the input layer\n",
    "cnn.add(tf.keras.layers.Conv2D(filters=32,kernel_size=3, activation='relu'))\n",
    "cnn.add(tf.keras.layers.MaxPool2D(pool_size=2, strides=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5939293e",
   "metadata": {},
   "source": [
    "# 3. Flattening"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a212d667",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flatten the CN ouput which can be input a ANN\n",
    "cnn.add(tf.keras.layers.Flatten())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bad7b311",
   "metadata": {},
   "source": [
    "# 4. Full connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f367fa3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn.add(tf.keras.layers.Dense(units=128, activation = 'relu'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "086bdd5b",
   "metadata": {},
   "source": [
    "# 5. Output Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1ffb8d5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn.add(tf.keras.layers.Dense(units=1, activation = 'sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2e4fd142",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d (Conv2D)             (None, 62, 62, 32)        896       \n",
      "                                                                 \n",
      " max_pooling2d (MaxPooling2D  (None, 31, 31, 32)       0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 29, 29, 32)        9248      \n",
      "                                                                 \n",
      " max_pooling2d_1 (MaxPooling  (None, 14, 14, 32)       0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 6272)              0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 128)               802944    \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 129       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 813,217\n",
      "Trainable params: 813,217\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "cnn.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb741d66",
   "metadata": {},
   "source": [
    "# 6. Training the cnn model "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f374191f",
   "metadata": {},
   "source": [
    "### Compiling the CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ebdeeac6",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn.compile(optimizer='adam', loss = 'binary_crossentropy', metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22eb670a",
   "metadata": {},
   "source": [
    "### Trianing on training data and evaluate on test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "74af3bfe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n",
      "250/250 [==============================] - 588s 2s/step - loss: 0.6906 - accuracy: 0.5444 - val_loss: 0.6579 - val_accuracy: 0.6180\n",
      "Epoch 2/25\n",
      "250/250 [==============================] - 118s 471ms/step - loss: 0.6503 - accuracy: 0.6226 - val_loss: 0.6255 - val_accuracy: 0.6550\n",
      "Epoch 3/25\n",
      "250/250 [==============================] - 179s 718ms/step - loss: 0.6256 - accuracy: 0.6522 - val_loss: 0.6000 - val_accuracy: 0.6900\n",
      "Epoch 4/25\n",
      "250/250 [==============================] - 97s 388ms/step - loss: 0.5858 - accuracy: 0.6917 - val_loss: 0.5603 - val_accuracy: 0.7190\n",
      "Epoch 5/25\n",
      "250/250 [==============================] - 54s 215ms/step - loss: 0.5532 - accuracy: 0.7234 - val_loss: 0.5532 - val_accuracy: 0.7190\n",
      "Epoch 6/25\n",
      "250/250 [==============================] - 55s 218ms/step - loss: 0.5247 - accuracy: 0.7377 - val_loss: 0.5288 - val_accuracy: 0.7325\n",
      "Epoch 7/25\n",
      "250/250 [==============================] - 60s 241ms/step - loss: 0.5052 - accuracy: 0.7513 - val_loss: 0.5355 - val_accuracy: 0.7465\n",
      "Epoch 8/25\n",
      "250/250 [==============================] - 55s 221ms/step - loss: 0.4803 - accuracy: 0.7685 - val_loss: 0.5265 - val_accuracy: 0.7470\n",
      "Epoch 9/25\n",
      "250/250 [==============================] - 86s 345ms/step - loss: 0.4580 - accuracy: 0.7836 - val_loss: 0.4786 - val_accuracy: 0.7775\n",
      "Epoch 10/25\n",
      "250/250 [==============================] - 95s 378ms/step - loss: 0.4457 - accuracy: 0.7864 - val_loss: 0.4807 - val_accuracy: 0.7780\n",
      "Epoch 11/25\n",
      "250/250 [==============================] - 85s 342ms/step - loss: 0.4265 - accuracy: 0.8027 - val_loss: 0.5060 - val_accuracy: 0.7630\n",
      "Epoch 12/25\n",
      "250/250 [==============================] - 323s 1s/step - loss: 0.4068 - accuracy: 0.8146 - val_loss: 0.4646 - val_accuracy: 0.7850\n",
      "Epoch 13/25\n",
      "250/250 [==============================] - 104s 415ms/step - loss: 0.3999 - accuracy: 0.8152 - val_loss: 0.4615 - val_accuracy: 0.7890\n",
      "Epoch 14/25\n",
      "250/250 [==============================] - 67s 267ms/step - loss: 0.3793 - accuracy: 0.8298 - val_loss: 0.5085 - val_accuracy: 0.7645\n",
      "Epoch 15/25\n",
      "250/250 [==============================] - 59s 235ms/step - loss: 0.3609 - accuracy: 0.8391 - val_loss: 0.4856 - val_accuracy: 0.7815\n",
      "Epoch 16/25\n",
      "250/250 [==============================] - 58s 231ms/step - loss: 0.3523 - accuracy: 0.8464 - val_loss: 0.4708 - val_accuracy: 0.7795\n",
      "Epoch 17/25\n",
      "250/250 [==============================] - 83s 331ms/step - loss: 0.3358 - accuracy: 0.8512 - val_loss: 0.5211 - val_accuracy: 0.7785\n",
      "Epoch 18/25\n",
      "250/250 [==============================] - 91s 365ms/step - loss: 0.3232 - accuracy: 0.8609 - val_loss: 0.4672 - val_accuracy: 0.7975\n",
      "Epoch 19/25\n",
      "250/250 [==============================] - 74s 295ms/step - loss: 0.3076 - accuracy: 0.8680 - val_loss: 0.4637 - val_accuracy: 0.8100\n",
      "Epoch 20/25\n",
      "250/250 [==============================] - 138s 554ms/step - loss: 0.2963 - accuracy: 0.8720 - val_loss: 0.5129 - val_accuracy: 0.7945\n",
      "Epoch 21/25\n",
      "250/250 [==============================] - 204s 816ms/step - loss: 0.2818 - accuracy: 0.8823 - val_loss: 0.4772 - val_accuracy: 0.7950\n",
      "Epoch 22/25\n",
      "250/250 [==============================] - 65s 260ms/step - loss: 0.2653 - accuracy: 0.8860 - val_loss: 0.4908 - val_accuracy: 0.8040\n",
      "Epoch 23/25\n",
      "250/250 [==============================] - 83s 334ms/step - loss: 0.2562 - accuracy: 0.8935 - val_loss: 0.4993 - val_accuracy: 0.8015\n",
      "Epoch 24/25\n",
      "250/250 [==============================] - 62s 247ms/step - loss: 0.2411 - accuracy: 0.8995 - val_loss: 0.5120 - val_accuracy: 0.8005\n",
      "Epoch 25/25\n",
      "250/250 [==============================] - 66s 262ms/step - loss: 0.2179 - accuracy: 0.9124 - val_loss: 0.5664 - val_accuracy: 0.7855\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x21157cfccd0>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cnn.fit(x =  train_generator, validation_data = test_set, epochs=25 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "184662eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from keras_preprocessing import image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "925c90a7",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'dataset/single_prediction/cat_or_dog_1.jpg'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-19-0d96b5216f03>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# Load the single image we want to predict\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mtest_image\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mimage\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload_img\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'dataset/single_prediction/cat_or_dog_1.jpg'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_size\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;36m64\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m64\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mtest_image\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mimage\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mimg_to_array\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_image\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mtest_image\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexpand_dims\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_image\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_image\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras_preprocessing\\image\\utils.py\u001b[0m in \u001b[0;36mload_img\u001b[1;34m(path, grayscale, color_mode, target_size, interpolation)\u001b[0m\n\u001b[0;32m    111\u001b[0m         raise ImportError('Could not import PIL.Image. '\n\u001b[0;32m    112\u001b[0m                           'The use of `load_img` requires PIL.')\n\u001b[1;32m--> 113\u001b[1;33m     \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'rb'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    114\u001b[0m         \u001b[0mimg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpil_image\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mio\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mBytesIO\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    115\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcolor_mode\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'grayscale'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'dataset/single_prediction/cat_or_dog_1.jpg'"
     ]
    }
   ],
   "source": [
    "# Load the single image we want to predict \n",
    "test_image = image.load_img('dataset/single_prediction/cat_or_dog_1.jpg', target_size = (64,64))\n",
    "test_image = image.img_to_array(test_image)\n",
    "test_image = np.expand_dims(test_image, axis = 0)\n",
    "result = cnn.predict(test_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da293a50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classes 1 corresponds to dog and 0 to cats \n",
    "train_generator.class_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b2494dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "result[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "787ec670",
   "metadata": {},
   "outputs": [],
   "source": [
    "if result[0][0] >=  .5: \n",
    "    prediction = 'dog'\n",
    "elif result[0][0] < .5:\n",
    "    prediction = 'cat'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eba15f22",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1099716",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
